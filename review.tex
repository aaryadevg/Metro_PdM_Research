\documentclass{article}

\title{Failure prediction for APU's on a Metro System}
\author{Aaryadev Ghosalkar}
\date{\today}

\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage{graphicx}
\graphicspath{ {./} }
\usepackage{amsmath,amssymb}

\begin{document}

\maketitle

\begin{abstract}
Predictive maintenance is vital for ensuring the reliable operation of metro transportation systems, enhancing passenger safety, and minimizing service disruptions. In this research, we focus on the challenging task of predicting failures on the Air Processing Unit (APU) on a metro system at least two hours in advance. An APU is a component on the metro responsible for compressing and storing compressed air for use on various functions on the metro such as the suspension. Our study encompasses a comprehensive comparison of various machine learning models, including Support Vector Machines (SVM), Convolutional Neural Networks (CNN), Long Short-Term Memory (LSTM) networks, and transformer-based models. These models are evaluated for their performance in predicting impending failures using various metrics such as accuracy, precision, recall and F1-Score for classification and Mean Squared Error (MSE) for Regression, providing valuable insights into their applicability in this context.

Additionally, we introduce a practical demonstration of a web API designed to showcase how the model would be deployed in a real world situation. This API serves as a testament to the feasibility of deploying predictive maintenance models in production settings.

It is important to note that while our research offers valuable insights into model performance and deployment readiness, we refrain from proposing a final production model. This limitation arises from the complex domain knowledge required for selecting an optimal model and the to keep the paper more general since the requirements of various transportation companies need not be the same. Nevertheless, our findings contribute to the ongoing efforts to bolster metro system reliability and safety through proactive failure prediction. \\

\textbf{Keywords:} Metro system, predictive maintenance, failure prediction, machine learning, deep learning, model deployment.
\end{abstract}

\newpage

\section{Literature Review}

The initial phase of data extraction was
undertaken by the work conducted by Veloloso et al \cite{Veloso2022}. In their work they expounded upon the operational principles of diverse sensors
and articulated the central objective of predicting metro system failures. 
Moreover, they explained the precise criteria stipulated by the railway company for the assessment of the predictive model, thus serving as an instrumental starting point for the subsequent research endeavor. \\

The findings of Azure ML Team at Microsoft \cite{AzureML2015}, show that there exist two primary approaches to tackle our problem: binary classification or failure prediction, where the objective is to predict whether the metro system will fail within the next two hours (yes or no), and regression, which in the context of our research pertains to predicting the number of cycles or amount of time before the metro system experiences a failure, or needs to be repaired. Also known as Remaining Useful life (RUL) Prediction. This paper will undertake a comparative analysis of both these approaches. \\

Davari et al \cite{Davari2021} have conducted a survey on data-driven or Machine Learning (ML) and Deep Learning (DL) approaches to PdM. In their work they have compared both approaches to we will expand on this by providing more insight into which approaches are viable in the context for metro systems.

\subsection{Failure Prediction}

Numerous traditional ML algorithms are applied in the realm of failure prediction. As detailed in the works of Chaudhuri et al \cite{chaudhuri2018}, where Support Vector Machines (SVM) and various SVM variants were employed to classify vehicles into three distinct risk categories: Immediate, Long-term, and Medium-term. This approach is simple and the results are easy to understand. \\

Convolution Neural Networks (CNN) can be applied to time series data, typically used for
images, requires a method for converting time series into image-like representations using GAF (Gramian Angular Field) feature transformation, which facilitates the conversion of one-dimensional time series data into images. Leveraging this transformation for PdM Silva reported an accuracy rate of 93\% on the SF103 dataset \cite{Silva2019}. \\

Long Short-Term Memory (LSTM) networks have gained prominence for failure prediction. Nguyen and Medjaher employed LSTM to predict failures by quantifying the probability of system failure within a specified time window \cite{nguyen2019}. This approach holds promise for our problem, as it not only offers the potential for interpretability but also grants flexibility to the company in defining the severity thresholds for failure prediction.

\subsection{Remaining Useful Life Estimation}

An alternative perspective on the problem involves estimating the Remaining Useful Life (RUL), which offers a versatile approach. This approach allows for the setting of thresholds to define significant risk levels based on the transportation companies needs this is usually done by monitoring specific variables that serve as reliable indicators of system deterioration however this approach requires domain knowledge about the features that can be used to predict the degradation of the system, due to this limitation we will be calculating the time to failure, the number of hours before failure based on the known failures. \\

Prior to 2011, a significant portion of research in Remaining Useful Life (RUL) prediction predominantly relied on statistical methods. Si et al.'s study exemplifies the utilization of various statistical techniques that were commonly employed, including linear regression and probabilistic models such as Markov models, in the context of RUL prediction \cite{SI2011}. \\

In 2016 with the rise of Deep Learning, Wang et al. compared a more data driven approach such as Deep Neural Networks (DNN) and Shallow Neural Networks (SNN) in predicting lubricant pressure for wind turbines within a farm \cite{wang2016}. It's worth noting that selecting the appropriate variable for monitoring often requires domain expertise.As mention earlierwe will take a slightly different approach in our study thus circumventing the need for domain-specific knowledge in variable selection. \\

Several models originally designed for failure prediction have found applicability in RUL estimation. For instance, Boujamza and Elhaq explored the utilization of an attention-based LSTM model for RUL estimation using the C-MAPSS Dataset \cite{Boujamza2022}. This adaptation suggests the possibility of exploring other attention-based models, such as transformers, for similar tasks in RUL estimation, expanding the scope of potential approaches for our research.

\newpage

\section{Background}

The dataset used in this study was provided by a company and contains sensor readings from the APU on metro trains. The data consists of sensor readings collected at a frequency of 1 Hz (1 reading per second) from 20 sensors within the APU. True labels for the observed events are also included in the dataset. The following features have been extracted from the data for analysis:

\begin{table}[htbp]
\centering
\caption{Summary Statistics and Data Types of Sensor Readings}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
Column & Data Type & Mean & Median & Min & Max \\
\hline
timestamp & DateTime & NA & NA & NA & NA \\
TP2 & Float & 1.0103 & -0.008 & -0.03 & 10.806 \\
TP3 & Float & 8.9730 & 8.99 & 0.938 & 10.38 \\
H1 & Float & 7.9479 & 8.86 & -0.034 & 10.368 \\
DV\_Pressure & Float & -0.0192 & -0.026 & -0.036 & 8.11 \\
Reservoirs & Float & 1.5892 & 1.606 & 1.35 & 1.726 \\
Oil\_Temperature & Float & 66.2864 & 67.2 & 20.8 & 80.175 \\
Flowmeter & Float & 20.2065 & 19.0123 & 18.8347 & 37.0083 \\
Motor\_Current & Float & 2.1342 & 3.665 & -0.01 & 9.3375 \\
COMP & Bool & 0.8829 & 1 & 0 & 1 \\
DV\_Electric & Bool & 0.1171 & 0 & 0 & 1 \\
TOWERS & Bool & 0.9413 & 1 & 0 & 1 \\
MPG & Bool & 0.8829 & 1 & 0 & 1 \\
LPS & Bool & 0.0079 & 0 & 0 & 1 \\
Pressure\_Switch & Bool & 0 & 0 & 0 & 0 \\
Oil\_Level & Bool & 0.0031 & 0 & 0 & 0 \\
Caudal\_Impulses & Bool & -7.7623 & 0 & 0 & 1 \\
gpsLat & Float & 37.0052 & -8.6583 & -8.6941 & 41.2401 \\
gpsLon & Float & 9.9405 & 41.1855 & 0 & 216 \\
gpsSpeed & Float & 0.8984 & 0 & 0 & 1 \\
gpsSignal & Float & NA & 1 & 0 & 1 \\
\hline
\end{tabular}
\end{table}

It is crucial to acknowledge that these values are approximations and are solely intended to provide a broad overview of the data distribution. Furthermore, it is essential to emphasize that the dataset is does not contain any missing values. Additional information about the data is provided by the original authors veloso et al\cite{Veloso2022}

\newpage

\section{Data preprocessing}

In the data preprocessing stage, two distinct copies of the dataset are being created, each tailored for a different Machine Learning (ML) task:

\subsection{Multiclass Classification} 

In this task, the primary objective is to classify data into three distinct states: normal operational conditions, pre-failure (signifying a state occurring two hours before an impending failure), and failure. A data point is designated as a "failure" state if it falls within the time window of documented failure events. Conversely, if a data point is at least two hours from any documented failure event, it is categorized as "normal" state. with these transformation we have a target column for a multiclass classification problem.\\ 

We choose this approach due to the inherent challenge of predicting a continuous timestamp directly using a machine learning model. Therefore, a process of discretization is essential. However, it is important to highlight the significance of the chosen threshold, denoted as $N_{Hours}$ .It is worth noting that setting this threshold too high may lead to a potential decrease in accuracy, as the model might encounter difficulties distinguishing between normal and pre-failure states. We use $N_{Hours} = 2$ since the dataset description mentioned the need to predict failures atleast 2 hours in advance \\

after applying the above mentioned transformation we obtain the following class distribution. \\

\begin{table}[htbp]
\centering
\caption{Class Distribution}
\begin{tabular}{|l|l|}
\hline
\multicolumn{1}{|c|}{\textbf{Class}} & \multicolumn{1}{c|}{\textbf{Instances}} \\ \hline
0 (Normal)                           & 10539882                                \\ \hline
1 (Pre Failure)                      & 212106                                  \\ \hline
2 (Failure)                          & 21600                                   \\ \hline
\end{tabular}
\end{table}

Table 2 reveals a significant imbalance within the dataset, which poses a risk of the model favoring one class and achieving a high accuracy by simply predicting that class. In our case there are 10 539 882 Data points which accounts for 97.83\% in other words if the model classifies all points as "Normal" the accuracy will be 97.8\%. \\

To address this issue, we must employ data resampling techniques. Undersampling involves reducing the number of instances in the majority class, while oversampling entails increasing the instances in the minority class. \\

\newpage

In our case, due to the dataset's substantial size and the pronounced class imbalance, we have choose undersampling. This decision is based on the understanding that oversampling, would generate many redundant data points, thereby increasing the risk of overfitting. \cite{ELLIS2023}.

\subsection{Regression} 
    
To prepare the data for regression, we calculated the temporal distance between each data point and the nearest failure event within the dataset. It is worth noting that this method, while serviceable, assumes a linear degradation pattern for components and may benefit from more sophisticated approximations to enhance model performance. Nevertheless, it serves as a foundational step in our analysis. done with the following algorithm.

\begin{algorithm}
\caption{Find Time Till Failure}
\begin{algorithmic}
\For{$(start, end)$ in $failure\_periods$}
    \If{$curr\_time < start$}
        \State $Hours\_till\_Failure \gets \frac{(start - curr\_time).total\_seconds()}{3600}$
        \State \textbf{return} $Hours\_till\_Failure$
    \EndIf
\EndFor
\State $Hours\_till\_Failure \gets 0$ \\
\For{each row in $Reg\_df$}
    \State $curr\_time \gets row["timestamp"]$
    \State $Reg\_df[row]["Hours\_till\_Failure"] \gets find\_time\_till\_failure(curr\_time)$
\EndFor
\end{algorithmic}
\end{algorithm}

Where $failure\_periods$ is a list of pairs $(start, end)$, it is important to note that $failure\_periods$ is sorted according to $start$ (The first element in the pair). \\

In the context of the regression approach, the use of undersampling is typically unnecessary, as it does not pertain to a classification problem. Instead, our primary focus is on evaluating the model's mean squared error (MSE). Given our objective of predicting a failure at least 2 hours in advance, we aim for the mean squared error to be approximately around 4 ($2^2$), which signifies that, on average, the model is deviating by 2 hours in its predictions. This MSE threshold of 4 serves as a meaningful benchmark for our regression model.

\newpage

\section{Machine Learning for Failure Prediction}

As mentioned in the abstract, the comparative analysis will encompass several machine learning (ML) models and deep learning models. It is important to note that all figures presented in this study are computed using the test subset of the resampled data. Specifically, 30\% of the entire resampled dataset was designated for testing purposes. Furthermore a random seed of 42 was employed to ensure consistency in the selection of the test data. \\

Prior to delving into the model evaluations and results, it is imperative to establish a theoretical baseline. This baseline ensures that the models have learned meaningful patterns. \\

We three classes, each having approximately equal instances after resampling, thus we can reasonably assume that the probabilities of encountering each class are roughly equal, denoted as $P(Class_1) \approx P(Class_2) \approx P(Class_3)$. therefore, a model making random guesses would have an expected accuracy of 0.33, as it should have a $\frac{1}{3}$ chance of correctly predicting the class. Therefore, any accuracy significantly surpassing 0.33 on the testing data would signify that the model has successfully identified discernible patterns and is not relying on random chance for predictions.\\

\subsection{Support Vector Machines}

\subsubsection{Background}

The Support Vector Machine (SVM) is a classification algorithm that seeks to delineate data points using a hyperplane. By default, SVM is utilized to establish a linear decision boundary. However, through using the kernel trick, where a kernel transformation is applied to all data points before training the SVM, enabling the model to learn non-linear decision boundaries \cite{Geron2019}. For this problem we are using Gaussian RBF kernel defined as follows:

\begin{equation}
    \phi_\gamma(x, l) = \exp (-\gamma \|x + l\|)
\end{equation}

In the absence of a kernel, the SVM classifies data points based on a decision function, which is defined as follows:

\begin{equation}
\hat{y} =
    \begin{cases}
        0 & \text{if } w^Tx + b < 0\\
        1 & \text{if } w^Tx + b \geq 0
    \end{cases}
\end{equation}

Where the weight vector $w$ controls the width of the decision boundary, the smaller the value of $w$ the larger the decision boundary, thus we would like to minimize $\|w\|$. However, if we also want to avoid
any margin violation (hard margin), then we need the decision function to be greater
than 1 for all positive training instances, and lower than –1 for negative training
instances. This denotes a Hard Margin SVM.\\

Typically, two types of SVMs are employed: hard margin and soft margin SVMs. In a hard margin SVM, no data points are permitted within the margin. In contrast, a soft margin SVM allows for data points to be situated within the margin, and the extent to which this is allowed is regulated by a hyperparameter denoted as $C$ A smaller value of $C$ enforces a stricter margin, and $C = 0$ signifies a hard margin, where no points are permitted within the margin \cite{Geron2019}. In your specific case, you have chosen $C = 1$. Hard margin SVM are more prone to overfitting. To train a softmargin SVM we generally add a slack variable to the minimization equation.

\subsubsection{Results}

\begin{table}[htbp]
\centering
\caption{SVM Results}
\begin{tabular}{|l|lll|}
\hline
                    & \multicolumn{1}{l|}{\textbf{Normal}} & \multicolumn{1}{l|}{\textbf{Pre Failure}} & \textbf{Failure} \\ \hline
\textbf{Precision}  & \multicolumn{1}{l|}{0.55}            & \multicolumn{1}{l|}{0.60}                 & 0.52             \\ \hline
\textbf{Recall}     & \multicolumn{1}{l|}{0.62}            & \multicolumn{1}{l|}{0.62}                 & 0.36             \\ \hline
\textbf{F1 - Score} & \multicolumn{1}{l|}{0.58}            & \multicolumn{1}{l|}{0.61}                 & 0.43             \\ \hline
\textbf{Accuracy}   & \multicolumn{3}{r|}{0.57}                                                                           \\ \hline
\end{tabular}
\end{table}

The SVM model, without any hyperparameter optimization, has the worst performance, with an accuracy of 57\%. However this accuracy is still higher than the theoretical baseline of 33.33\% established at the beginning of Section 4. which means even this model is learning some pattern.\\

In terms of precision for class 1 (Pre-Failure), the SVM achieved a precision of 0.6, indicating that it correctly classifies 60\% of instances as pre-failure. This performance is particularly useful for us.\\

It's worth noting that the SVM appeared to struggle with distinguishing between normal and pre-failure states, with a misclassification rate of approximately 32\%. Additionally, it exhibited limited success in correctly predicting actual failure cases, achieving a correct prediction rate of only about 36\%.\\

These observations are supported by the confusion matrix presented below, which highlights the model's performance with respect to different classes. \\

\begin{align*}
\begin{bmatrix}
0.61969674 & 0.26742227 & 0.11288099 \\
0.32976412 & 0.61530106 & 0.05493482 \\
0.37874659 & 0.26006661 & 0.3611868 \\
\end{bmatrix}
\end{align*}

\newpage

\subsection{Decision Trees}
\subsubsection{Background}

A decision tree is a widely used machine learning algorithm for classification tasks. It takes the form of a tree-like structure, where internal nodes represent decisions or tests based on input features, branches represent the outcomes of these decisions or tests, and leaf nodes signify class labels assigned to data instances.\\

It's essential to emphasize that, in theory, finding the most optimal decision tree is an NP-Complete problem. This means that, as the size and complexity of the dataset and the tree structure increase, the computational resources required to find the best tree become prohibitively large. NP-Complete problems are a class of problems for which no efficient polynomial time algorithm exists to find the exact optimal solution, making it necessary to rely on heuristics and approximations when building decision trees for practical applications \cite{laurent1976}.\\

Decision trees can be trained using various algorithms, and among them are CART (Classification and Regression Tree) and ID3 (Iterative Dichotomiser 3). For this study, the decision tree model will be constructed using the CART algorithm which has a cost function given by the following equation:

\begin{equation}
    J(k, t_k) = \frac{m_{left}}{m} \cdot G_{left} + \frac{m_{right}}{m} \cdot G_{right}
\end{equation}

Where $G_{left/right}$ is the Gini impurity and $m_{left/right}$ is the number of instances on left or right.\\

Gini Impurity is defined as follows:
\begin{equation}
    G_i = 1 - \sum_{k = 1}^{n}{P_{(i,k)}^2}
\end{equation}

Where $P_{(i,k)}$ is  is the ratio of class k instances among the training instances in the ith node

\subsubsection{Results}

\begin{table}[htbp]
\centering
\caption{Decision Tree Results}
\begin{tabular}{|l|lll|}
\hline
                    & \multicolumn{1}{l|}{\textbf{Normal}} & \multicolumn{1}{l|}{\textbf{Pre Failure}} & \textbf{Failure} \\ \hline
\textbf{Precision}  & \multicolumn{1}{l|}{0.70}            & \multicolumn{1}{l|}{0.69}                 & 0.51             \\ \hline
\textbf{Recall}     & \multicolumn{1}{l|}{0.68}            & \multicolumn{1}{l|}{0.70}                 & 0.52             \\ \hline
\textbf{F1 - Score} & \multicolumn{1}{l|}{0.69}            & \multicolumn{1}{l|}{0.70}                 & 0.52             \\ \hline
\textbf{Accuracy}   & \multicolumn{3}{r|}{0.66}                                                                           \\ \hline
\end{tabular}
\end{table}

The decision tree model represents an improvement over the SVM, achieving an accuracy of 66\%. The precision for pre-failure cases has also seen a substantial increase, reaching 69\%. Despite these improvements, the decision tree still faces challenges in reliably predicting failure cases. However, it's an improvement of almost 17\% compared to the SVM. \\

Additionally, the decision tree model demonstrates superior computational efficiency we will go over the computation complexity of the tested algorithms later, particularly for large datasets. It's important to mention that during hyperparameter tuning, different criteria were explored, including log\_loss, entropy, and Gini impurity, but the impact on model performance was relatively minor.\\

The confusion Matrix for the decision tree was as follows:

\begin{align*}
\begin{bmatrix}
0.68019605 & 0.18456119 & 0.13524276 \\
0.18171943 & 0.70251397 & 0.1157666 \\
0.22615804 & 0.25158946 & 0.5222525 \\
\end{bmatrix}
\end{align*}

\subsection{Random Forest}
\subsubsection{Background}

The Random Forest algorithm represents an improvement over  decision trees classifier. It is classified as an ensemble technique, which means it uses predictions from multiple models. Specifically, the Random Forest algorithm involves training a "forest" of numerous decision tree classifiers. In this instance, 100 different decision trees are used. During inference, the algorithm determines the most common class among the predictions.

\subsubsection{Results}

\begin{table}[htbp]
\centering
\caption{Random Forest Results}
\begin{tabular}{|l|lll|}
\hline
                    & \multicolumn{1}{l|}{\textbf{Normal}} & \multicolumn{1}{l|}{\textbf{Pre Failure}} & \textbf{Failure} \\ \hline
\textbf{Precision}  & \multicolumn{1}{l|}{0.73}            & \multicolumn{1}{l|}{0.69}                 & 0.70             \\ \hline
\textbf{Recall}     & \multicolumn{1}{l|}{0.72}            & \multicolumn{1}{l|}{0.78}                 & 0.54             \\ \hline
\textbf{F1 - Score} & \multicolumn{1}{l|}{0.72}            & \multicolumn{1}{l|}{0.73}                 & 0.61             \\ \hline
\textbf{Accuracy}   & \multicolumn{3}{r|}{0.70}                                                                           \\ \hline
\end{tabular}
\end{table}

The Random Forest indeed offers improvements over decision trees in various aspects. It has shown an increase in accuracy, precision, and recall.\\

During the hyperparameter optimization phase, a slight accuracy increase of 1\% was achieved by experimenting with different criteria, such as entropy.

\subsection{Neural Networks}
\subsubsection{Background}

\subsubsection{Results}

\subsection{LSTM Networks}
\subsubsection{Background}

\subsubsection{Results}

\section{Machine Learing for RUL Estimation}

\section{Deployment and future scope}



\newpage

\bibliographystyle{IEEEtran}
\bibliography{metropt}

\end{document}
