{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import NearMiss\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "\n",
    "DATA_PATH = pathlib.Path(\"../Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    10539882\n",
       "2      212106\n",
       "1       21600\n",
       "Name: Target, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_feather(DATA_PATH / \"Classification.feather\")\n",
    "df[\"Target\"].value_counts()\n",
    "\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\imblearn\\under_sampling\\_prototype_selection\\_nearmiss.py:203: UserWarning: The number of the samples to be selected is larger than the number of samples available. The balancing ratio cannot be ensure and all samples will be returned.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X = df.drop(columns=[\"timestamp\", \"Target\"]) # this is purely a classification no time steps are needed\n",
    "y = df[\"Target\"]\n",
    "\n",
    "nm_undersampler = NearMiss(version=3, n_neighbors_ver3=3, n_jobs=-1) # Warning takes very long to run\n",
    "X, y = nm_undersampler.fit_resample(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = SVC(kernel=\"rbf\")\n",
    "svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.62      0.58      6529\n",
      "           1       0.60      0.62      0.61      6444\n",
      "           2       0.52      0.36      0.43      3303\n",
      "\n",
      "    accuracy                           0.57     16276\n",
      "   macro avg       0.56      0.53      0.54     16276\n",
      "weighted avg       0.56      0.57      0.56     16276\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.61969674, 0.26742227, 0.11288099],\n",
       "       [0.32976412, 0.61530106, 0.05493482],\n",
       "       [0.37874659, 0.26006661, 0.3611868 ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred, normalize=\"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Baseline\n",
    "\n",
    "There are 3 classes and we are balancing the dataset such that each class has a each have roughly same number of instances therefore $P(Class_1) \\approx P(Class_2) \\approx P(Class_3)$ which means a model making pure random guesses should have a $\\frac{1}{3}$ chance of getting the right class so anything significantly above a 0.33 accuracy on the testing data means that the model has found some pattern and is mot guessing randomly\n",
    "\n",
    "## SVM\n",
    "\n",
    "- Accuracy is 57%\n",
    "- Precision for class 1 (Pre Failure) is 0.6, 60% of instances are correctly classified as pre failure\n",
    "- Getting confused between normal and pre failure at least 32% of the time\n",
    "- It is not too good at predicting an actually failure interestingly enough\n",
    "\n",
    "## Decision Tree (Hyper parameter have no significant differences)\n",
    "\n",
    "- 66% Accuracy (Good)\n",
    "- 69% of pre failures are correct (Nice 😉)\n",
    "- Predicting an actual failure is still a coin toss (49%)\n",
    "- Precision for class 1 is improvement 0.7\n",
    "- Trains much much quicker than SVM and is more accurate\n",
    "\n",
    "## Random Forest\n",
    "\n",
    "- Slight improvement over Decision Tree 69% (Nice 😉)\n",
    "- 1% decrease in predicting pre failures (68%)\n",
    "- Slightly improved at predicting actual failures but still pretty much a coin toss (52%)\n",
    "- Using Entropy is better, 1 addition percent accuracy\n",
    "\n",
    "## Neural Nets\n",
    "\n",
    "- Mostly Accurate (72%)\n",
    "- It's Really good a predicting pre failures (90%)\n",
    "- Still not very good at predicting actual failures\n",
    "- Training time better than SVM But also use CUDA so it's not very fair\n",
    "- Fairly small model is still doing good (25k Param)\n",
    "\n",
    "## Neural Net After some optimization\n",
    "\n",
    "- Beats LSTM in accuracy (76%)\n",
    "- 75% correct in predicting a pre-failure\n",
    "- Good at distinguishing all classes\n",
    "- Same model size, however was trained for longer\n",
    "- Optimizer was changed to AdamW\n",
    "\n",
    "## LSTM\n",
    "\n",
    "- Use with caution (really hard to evaluate) the average balanced acc = 0.74 (explain in paper)\n",
    "- Training time is long\n",
    "- largest model in comparison 2.4M parameters\n",
    "- Need a better way to compare these models\n",
    "\n",
    "In my Opinion LSTM seems un trustworthy, it is most accurate but we only know the accuracy no precision, recall or f1-score it's also not very easy to compute the other metrics and need significant time and computation power, using this in production needs much more research and would also need a lot more resources, I think for API development point of view the classic neural network is a good choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(criterion=&#x27;entropy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(criterion=&#x27;entropy&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(criterion=\"entropy\")\n",
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_dt = dt.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.68      0.69      6529\n",
      "           1       0.69      0.70      0.70      6444\n",
      "           2       0.51      0.52      0.52      3303\n",
      "\n",
      "    accuracy                           0.66     16276\n",
      "   macro avg       0.63      0.63      0.63     16276\n",
      "weighted avg       0.66      0.66      0.66     16276\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.68019605, 0.18456119, 0.13524276],\n",
       "       [0.18171943, 0.70251397, 0.1157666 ],\n",
       "       [0.22615804, 0.25158946, 0.5222525 ]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred, normalize=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.69      0.69      6529\n",
      "           1       0.69      0.71      0.70      6444\n",
      "           2       0.53      0.49      0.51      3303\n",
      "\n",
      "    accuracy                           0.66     16276\n",
      "   macro avg       0.63      0.63      0.63     16276\n",
      "weighted avg       0.65      0.66      0.66     16276\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.68816051, 0.18869658, 0.1231429 ],\n",
       "       [0.18885785, 0.70732464, 0.1038175 ],\n",
       "       [0.25158946, 0.25491977, 0.49349077]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(criterion=\"log_loss\")\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_dt))\n",
    "confusion_matrix(y_test, y_pred_dt, normalize=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.72      0.71      6529\n",
      "           1       0.68      0.76      0.72      6444\n",
      "           2       0.68      0.52      0.59      3303\n",
      "\n",
      "    accuracy                           0.69     16276\n",
      "   macro avg       0.69      0.67      0.67     16276\n",
      "weighted avg       0.69      0.69      0.69     16276\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.71634247, 0.2014091 , 0.08224843],\n",
       "       [0.20173805, 0.75713842, 0.04112353],\n",
       "       [0.19194672, 0.28580079, 0.5222525 ]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(criterion=\"gini\", random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "confusion_matrix(y_test, y_pred_rf, normalize=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.72      0.72      6529\n",
      "           1       0.69      0.78      0.73      6444\n",
      "           2       0.70      0.54      0.61      3303\n",
      "\n",
      "    accuracy                           0.70     16276\n",
      "   macro avg       0.70      0.68      0.69     16276\n",
      "weighted avg       0.71      0.70      0.70     16276\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.71634247, 0.20554449, 0.07811303],\n",
       "       [0.1801676 , 0.77855369, 0.04127871],\n",
       "       [0.17741447, 0.28670905, 0.53587648]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(criterion=\"entropy\", random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "confusion_matrix(y_test, y_pred_rf, normalize=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.72      0.72      6529\n",
      "           1       0.69      0.78      0.73      6444\n",
      "           2       0.70      0.54      0.61      3303\n",
      "\n",
      "    accuracy                           0.70     16276\n",
      "   macro avg       0.70      0.68      0.69     16276\n",
      "weighted avg       0.71      0.70      0.70     16276\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.71634247, 0.20554449, 0.07811303],\n",
       "       [0.1801676 , 0.77855369, 0.04127871],\n",
       "       [0.17741447, 0.28670905, 0.53587648]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(criterion=\"log_loss\", random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "confusion_matrix(y_test, y_pred_rf, normalize=\"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying a neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import lightning.pytorch as pl\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from typing import Any\n",
    "from lightning.pytorch.utilities.types import STEP_OUTPUT, OptimizerLRScheduler\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([37976, 16])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the NumPy arrays to PyTorch tensors\n",
    "X_train_t = torch.from_numpy(X_train)\n",
    "y_train_t = torch.from_numpy(y_train.to_numpy())\n",
    "X_test_t = torch.from_numpy(X_test)\n",
    "y_test_t = torch.from_numpy(y_test.to_numpy())\n",
    "\n",
    "# Create the dataset object\n",
    "train_ds = TensorDataset(X_train_t, y_train_t)\n",
    "test_ds = TensorDataset(X_test_t, y_test_t)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32)\n",
    "test_loader = DataLoader(test_ds, batch_size=32)\n",
    "\n",
    "X_train_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_Classifier(pl.LightningModule):\n",
    "    def __init__(self, n_inputs) -> None:\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(16, 256),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 3)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx) -> STEP_OUTPUT:\n",
    "        X, y = batch\n",
    "        y_pred = self(X)\n",
    "        loss = F.cross_entropy(y_pred, y)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx) -> STEP_OUTPUT:\n",
    "        X, y = batch\n",
    "        y_pred = self(X)\n",
    "        y_lab = torch.argmax(F.softmax(y_pred, dim=1), dim=1)\n",
    "        \n",
    "        correct = (y_lab == y).sum()\n",
    "        acc = correct / X.shape[0]\n",
    "        \n",
    "        loss = F.cross_entropy(y_pred, y)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, on_epoch=True)\n",
    "        self.log(\"val_acc\", acc, prog_bar=True, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self) -> OptimizerLRScheduler:\n",
    "        return torch.optim.Adam(self.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\loops\\utilities.py:72: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | Sequential | 25.2 K\n",
      "-------------------------------------\n",
      "25.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "25.2 K    Total params\n",
      "0.101     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|██████████| 1187/1187 [00:07<00:00, 151.76it/s, v_num=5, val_loss=0.592, val_acc=0.716]\n"
     ]
    }
   ],
   "source": [
    "model = NN_Classifier(X_train_t.shape[1])\n",
    "es_callback = EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=3, strict=True)\n",
    "\n",
    "trainer = pl.Trainer(accelerator=\"gpu\", callbacks=[es_callback])\n",
    "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.66      0.72      6529\n",
      "           1       0.67      0.91      0.77      6444\n",
      "           2       0.72      0.46      0.56      3303\n",
      "\n",
      "    accuracy                           0.72     16276\n",
      "   macro avg       0.72      0.67      0.68     16276\n",
      "weighted avg       0.73      0.72      0.71     16276\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model(X_test_t)\n",
    "y_hat = torch.argmax(F.softmax(y_pred, dim=1), dim=1).detach().cpu().numpy()\n",
    "y_true = y_test_t.numpy()\n",
    "\n",
    "print(classification_report(y_true, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.66089753, 0.2660438 , 0.07305866],\n",
       "       [0.07479826, 0.90657976, 0.01862197],\n",
       "       [0.20829549, 0.33636088, 0.45534363]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_true, y_hat, normalize=\"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Neural networks\n",
    "\n",
    "- Context length = 84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import lightning.pytorch as pl\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from typing import Any\n",
    "from lightning.pytorch.utilities.types import STEP_OUTPUT, OptimizerLRScheduler\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = pathlib.Path(\"../Data\")\n",
    "df = pd.read_feather(DATA_PATH / \"Classification.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"timestamp\", \"Target\"]).to_numpy()\n",
    "y = df[\"Target\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10773588, 16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 84\n",
    "nSplits = X.shape[0] // SEQ_LEN\n",
    "X_t = np.stack(np.array_split(X, nSplits), axis=0).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_t = y[SEQ_LEN - 1::SEQ_LEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128257, (84, 16))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_t), X_t[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128257"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128257, 84, 16)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With LSTM the input is 3rd Order Tensor of shape (nSplits, SEQ_LEN, n_Features) where \n",
    "- nSplits    = 128_257\n",
    "- SEQ_LEN    = 84\n",
    "- n_Features = 16\n",
    "\n",
    "So we can't actually re sample this data, thus the accuracy should be taken with some scrutiny, just predicting everything is ok will lead to an accuracy of about 97% so we need some kind of a weighted accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_t, y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([96192, 84, 16])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_t = torch.from_numpy(X_train)\n",
    "y_train_t = torch.from_numpy(y_train)\n",
    "X_test_t = torch.from_numpy(X_test)\n",
    "y_test_t = torch.from_numpy(y_test)\n",
    "\n",
    "# Create the dataset object\n",
    "train_ds = TensorDataset(X_train_t, y_train_t)\n",
    "test_ds = TensorDataset(X_test_t, y_test_t)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, num_workers=1, persistent_workers=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=32, num_workers=1, persistent_workers=True)\n",
    "\n",
    "X_train_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Classifer(pl.LightningModule):\n",
    "    def __init__(self, n_features = 16, n_classes = 3, n_hidden = 64, n_layers = 3) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size  = n_features,\n",
    "            hidden_size = n_hidden,\n",
    "            num_layers  = n_layers,\n",
    "            dropout     = 0.8,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Linear(n_hidden, n_classes)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        _ , (out, _) = self.lstm(x)\n",
    "        return self.classifier(out[-1])\n",
    "    \n",
    "    def training_step(self, batch, batch_idx) -> STEP_OUTPUT:\n",
    "        X, y = batch\n",
    "        y_pred = self(X)\n",
    "        loss = F.cross_entropy(y_pred, y)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx) -> STEP_OUTPUT:\n",
    "        X, y = batch\n",
    "        y_pred = self(X)\n",
    "        y_lab = torch.argmax(F.softmax(y_pred, dim=1), dim=1).detach().cpu().numpy()\n",
    "        loss = F.cross_entropy(y_pred, y)\n",
    "        \n",
    "        acc = balanced_accuracy_score(y.detach().cpu().numpy(), y_lab)\n",
    "        \n",
    "        self.log(\"val_loss\", loss, prog_bar=True, on_epoch=True)\n",
    "        self.log(\"val_acc\", acc, prog_bar=True, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self) -> OptimizerLRScheduler:\n",
    "        return torch.optim.Adam(self.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type   | Params\n",
      "--------------------------------------\n",
      "0 | lstm       | LSTM   | 87.6 K\n",
      "1 | classifier | Linear | 195   \n",
      "--------------------------------------\n",
      "87.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "87.7 K    Total params\n",
      "0.351     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 3006/3006 [00:24<00:00, 123.68it/s, v_num=12, val_loss=0.112, val_acc=0.743]\n"
     ]
    }
   ],
   "source": [
    "model = LSTM_Classifer()\n",
    "es_callback = EarlyStopping(monitor=\"val_loss\", min_delta=0.01, patience=3, strict=True)\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "pl.seed_everything(42)\n",
    "\n",
    "trainer = pl.Trainer(accelerator=\"gpu\", callbacks=[es_callback], default_root_dir=\"/LSTM_model\", max_epochs=25)\n",
    "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[31358     0     0]\n",
      " [   68     0     0]\n",
      " [  628     0    11]] 0.7429378531073447\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, confusion_matrix\n",
    "\n",
    "model.eval()\n",
    "\n",
    "accs: list = []\n",
    "\n",
    "classes = np.arange(3)\n",
    "\n",
    "global_cm : np.ndarray = np.zeros((3,3), dtype=np.int64)\n",
    "\n",
    "for b in test_loader:\n",
    "    cX, cy = b\n",
    "    pred = torch.argmax(F.softmax(model(cX), dim=1), dim=1).detach().cpu().numpy()\n",
    "    true = cy.detach().cpu().numpy()\n",
    "    acc = balanced_accuracy_score(true, pred)\n",
    "    accs.append(acc)\n",
    "    \n",
    "    cm = confusion_matrix(true, pred, labels=classes)\n",
    "    global_cm += cm\n",
    "    \n",
    "\n",
    "print(global_cm, np.mean(accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(cm: np.ndarray) -> np.ndarray:\n",
    "    return cm.diagonal() / cm.sum(axis=1)\n",
    "\n",
    "def recall(cm: np.ndarray) -> np.ndarray:\n",
    "    return cm.diagonal() / cm.sum(axis=0)\n",
    "\n",
    "def f1_score(cm) -> np.ndarray:\n",
    "    prec = precision(cm)\n",
    "    recl = recall(cm)\n",
    "    return (2 * prec * recl) / (prec + recl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision         = [1.        0.        0.0172144]\n",
      "recall            = [0.97828664        nan 1.        ]\n",
      "F1-Score          = [0.98902416        nan 0.03384615]\n",
      "Balanced Accuracy = 74.2938%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_13112\\705025223.py:5: RuntimeWarning: invalid value encountered in divide\n",
      "  return cm.diagonal() / cm.sum(axis=0)\n"
     ]
    }
   ],
   "source": [
    "print(f\"precision         = {precision(global_cm)}\")\n",
    "print(f\"recall            = {recall(global_cm)}\")\n",
    "print(f\"F1-Score          = {f1_score(global_cm)}\")\n",
    "print(f\"Balanced Accuracy = {np.mean(accs):.4%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31369"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_cm.trace()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
